// Copyright 2020-2024 Consensys Software Inc.
// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.

// Code generated by gnark DO NOT EDIT

package mpcsetup

import (
	"bytes"
	"crypto/sha256"
	"errors"
	"fmt"
	"github.com/consensys/gnark/backend/groth16/internal"
	"math/big"
	"slices"

	curve "github.com/consensys/gnark-crypto/ecc/bn254"
	"github.com/consensys/gnark-crypto/ecc/bn254/fr"
	"github.com/consensys/gnark/constraint"
	cs "github.com/consensys/gnark/constraint/bn254"
)

type Phase2Evaluations struct {
	G1 struct {
		A   []curve.G1Affine   // A are the left coefficient polynomials for each witness element, evaluated at τ
		B   []curve.G1Affine   // B are the right coefficient polynomials for each witness element, evaluated at τ
		VKK []curve.G1Affine   // VKK are the coefficients of the public witness and commitments
		CKK [][]curve.G1Affine // CKK are the coefficients of the committed values
	}
	G2 struct {
		B []curve.G2Affine // B are the right coefficient polynomials for each witness element, evaluated at τ
	}
}

type Phase2 struct {
	Parameters struct {
		G1 struct {
			Delta    curve.G1Affine
			Z        []curve.G1Affine   // Z are multiples of the domain vanishing polynomial
			PKK      []curve.G1Affine   // PKK are the coefficients of the private witness
			SigmaCKK [][]curve.G1Affine // Commitment bases
		}
		G2 struct {
			Delta curve.G2Affine
			Sigma []curve.G2Affine
		}
	}

	// Proofs of update correctness
	Sigmas []valueUpdate
	Delta  valueUpdate

	// Challenge is the hash of the PREVIOUS contribution
	Challenge []byte
}

func (c *Phase2) Verify(next *Phase2) error {
	if challenge := c.hash(); len(next.Challenge) != 0 && !bytes.Equal(next.Challenge, challenge) {
		return errors.New("the challenge does not match the previous phase's hash")
	}

	if err := next.Delta.verify(
		pair{c.Parameters.G1.Delta, &c.Parameters.G2.Delta},
		pair{next.Parameters.G1.Delta, &next.Parameters.G2.Delta},
		next.Challenge, 1); err != nil {
		return fmt.Errorf("failed to verify contribution to δ: %w", err)
	}

	for i := range c.Sigmas {
		if err := next.Sigmas[i].verify(
			pair{c.Parameters.G1.SigmaCKK[i][0], &c.Parameters.G2.Sigma[i]},
			pair{next.Parameters.G1.SigmaCKK[i][0], &next.Parameters.G2.Sigma[i]},
			next.Challenge, byte(2+i)); err != nil {
			return fmt.Errorf("failed to verify contribution to σ[%d]: %w", i, err)
		}
		if !areInSubGroupG1(next.Parameters.G1.SigmaCKK[i][1:]) {
			return errors.New("commitment proving key subgroup check failed")
		}
	}

	if !areInSubGroupG1(next.Parameters.G1.Z) || !areInSubGroupG1(next.Parameters.G1.PKK) {
		return errors.New("derived values 𝔾₁ subgroup check failed")
	}

	r := linearCombCoeffs(len(next.Parameters.G1.Z))

	for i := range c.Sigmas {
		prevComb, nextComb := linearCombination(c.Parameters.G1.SigmaCKK[i], next.Parameters.G1.SigmaCKK[i], r)
		if !sameRatioUnsafe(nextComb, prevComb, next.Parameters.G2.Sigma[i], c.Parameters.G2.Sigma[i]) {
			return fmt.Errorf("failed to verify contribution to σ[%d]", i)
		}
	}

	linearCombination()
}

func (c *Phase2) Contribute() {
	// Sample toxic δ
	var delta, deltaInv fr.Element
	var deltaBI, deltaInvBI big.Int

	c.Challenge = c.hash()

	if len(c.Parameters.G1.SigmaCKK) > 255 {
		panic("too many commitments") // DST collision
	}
	for i := range c.Parameters.G1.SigmaCKK {
		var (
			sigmaContribution  fr.Element
			sigmaContributionI big.Int
		)

		pk := c.Parameters.G1.SigmaCKK[i]
		c.Sigmas[i], sigmaContribution = updateValue(&pk[0], c.Challenge, byte(2+i))
		sigmaContribution.BigInt(&sigmaContributionI)
		for j := 1; j < len(pk); j++ {
			pk[j].ScalarMultiplication(&pk[j], &sigmaContributionI)
		}
		c.Parameters.G2.Sigma[i].ScalarMultiplication(&c.Parameters.G2.Sigma[i], &sigmaContributionI)
	}

	c.Delta, delta = updateValue(&c.Parameters.G1.Delta, c.Challenge, 1)

	deltaInv.Inverse(&delta)
	delta.BigInt(&deltaBI)
	deltaInv.BigInt(&deltaInvBI)

	// Update [δ]₂
	c.Parameters.G2.Delta.ScalarMultiplication(&c.Parameters.G2.Delta, &deltaBI)

	c.Parameters.G1.Delta.ScalarMultiplication(&c.Parameters.G1.Delta, &deltaBI)
	c.Parameters.G2.Delta.ScalarMultiplication(&c.Parameters.G2.Delta, &deltaBI)

	// Update Z using δ⁻¹
	for i := 0; i < len(c.Parameters.G1.Z); i++ {
		c.Parameters.G1.Z[i].ScalarMultiplication(&c.Parameters.G1.Z[i], &deltaInvBI)
	}

	// Update PKK using δ⁻¹
	for i := 0; i < len(c.Parameters.G1.PKK); i++ {
		c.Parameters.G1.PKK[i].ScalarMultiplication(&c.Parameters.G1.PKK[i], &deltaInvBI)
	}
}

// Init is to be run by the coordinator
// It involves no coin tosses. A verifier should
// simply rerun all the steps
func (p *Phase2) Init(commons SrsCommons) {

}

func InitPhase2(r1cs *cs.R1CS, srs1 *Phase1) (Phase2, Phase2Evaluations) {

	srs := srs1.parameters
	size := len(srs.G1.AlphaTau)
	if size < r1cs.GetNbConstraints() {
		panic("Number of constraints is larger than expected")
	}

	var c2 Phase2

	accumulateG1 := func(res *curve.G1Affine, t constraint.Term, value *curve.G1Affine) {
		cID := t.CoeffID()
		switch cID {
		case constraint.CoeffIdZero:
			return
		case constraint.CoeffIdOne:
			res.Add(res, value)
		case constraint.CoeffIdMinusOne:
			res.Sub(res, value)
		case constraint.CoeffIdTwo:
			res.Add(res, value).Add(res, value)
		default:
			var tmp curve.G1Affine
			var vBi big.Int
			r1cs.Coefficients[cID].BigInt(&vBi)
			tmp.ScalarMultiplication(value, &vBi)
			res.Add(res, &tmp)
		}
	}

	accumulateG2 := func(res *curve.G2Affine, t constraint.Term, value *curve.G2Affine) {
		cID := t.CoeffID()
		switch cID {
		case constraint.CoeffIdZero:
			return
		case constraint.CoeffIdOne:
			res.Add(res, value)
		case constraint.CoeffIdMinusOne:
			res.Sub(res, value)
		case constraint.CoeffIdTwo:
			res.Add(res, value).Add(res, value)
		default:
			var tmp curve.G2Affine
			var vBi big.Int
			r1cs.Coefficients[cID].BigInt(&vBi)
			tmp.ScalarMultiplication(value, &vBi)
			res.Add(res, &tmp)
		}
	}

	// Prepare Lagrange coefficients of [τ...]₁, [τ...]₂, [ατ...]₁, [βτ...]₁
	coeffTau1 := lagrangeCoeffsG1(srs.G1.Tau, size)           // [L_{ω⁰}(τ)]₁, [L_{ω¹}(τ)]₁, ... where ω is a primitive sizeᵗʰ root of unity
	coeffTau2 := lagrangeCoeffsG2(srs.G2.Tau, size)           // [L_{ω⁰}(τ)]₂, [L_{ω¹}(τ)]₂, ...
	coeffAlphaTau1 := lagrangeCoeffsG1(srs.G1.AlphaTau, size) // [L_{ω⁰}(ατ)]₁, [L_{ω¹}(ατ)]₁, ...
	coeffBetaTau1 := lagrangeCoeffsG1(srs.G1.BetaTau, size)   // [L_{ω⁰}(βτ)]₁, [L_{ω¹}(βτ)]₁, ...

	nbInternal, nbSecret, nbPublic := r1cs.GetNbVariables()
	nWires := nbInternal + nbSecret + nbPublic
	var evals Phase2Evaluations
	evals.G1.A = make([]curve.G1Affine, nWires) // recall: A are the left coefficients in DIZK parlance
	evals.G1.B = make([]curve.G1Affine, nWires) // recall: B are the right coefficients in DIZK parlance
	evals.G2.B = make([]curve.G2Affine, nWires) // recall: A only appears in 𝔾₁ elements in the proof, but B needs to appear in a 𝔾₂ element so the verifier can compute something resembling (A.x).(B.x) via pairings
	bA := make([]curve.G1Affine, nWires)
	aB := make([]curve.G1Affine, nWires)
	C := make([]curve.G1Affine, nWires)

	i := 0
	it := r1cs.GetR1CIterator()
	for c := it.Next(); c != nil; c = it.Next() {
		// each constraint is sparse, i.e. involves a small portion of all variables.
		// so we iterate over the variables involved and add the constraint's contribution
		// to every variable's A, B, and C values

		// A
		for _, t := range c.L {
			accumulateG1(&evals.G1.A[t.WireID()], t, &coeffTau1[i])
			accumulateG1(&bA[t.WireID()], t, &coeffBetaTau1[i])
		}
		// B
		for _, t := range c.R {
			accumulateG1(&evals.G1.B[t.WireID()], t, &coeffTau1[i])
			accumulateG2(&evals.G2.B[t.WireID()], t, &coeffTau2[i])
			accumulateG1(&aB[t.WireID()], t, &coeffAlphaTau1[i])
		}
		// C
		for _, t := range c.O {
			accumulateG1(&C[t.WireID()], t, &coeffTau1[i])
		}
		i++
	}

	// Prepare default contribution
	_, _, g1, g2 := curve.Generators()
	c2.Parameters.G1.Delta = g1
	c2.Parameters.G2.Delta = g2

	// Build Z in PK as τⁱ(τⁿ - 1)  = τ⁽ⁱ⁺ⁿ⁾ - τⁱ  for i ∈ [0, n-2]
	// τⁱ(τⁿ - 1)  = τ⁽ⁱ⁺ⁿ⁾ - τⁱ  for i ∈ [0, n-2]
	n := len(srs.G1.AlphaTau)
	c2.Parameters.G1.Z = make([]curve.G1Affine, n)
	for i := 0; i < n-1; i++ { // TODO @Tabaie why is the last element always 0?
		c2.Parameters.G1.Z[i].Sub(&srs.G1.Tau[i+n], &srs.G1.Tau[i])
	}
	bitReverse(c2.Parameters.G1.Z)
	c2.Parameters.G1.Z = c2.Parameters.G1.Z[:n-1]

	commitments := r1cs.CommitmentInfo.(constraint.Groth16Commitments)

	evals.G1.CKK = make([][]curve.G1Affine, len(commitments))
	c2.Sigmas = make([]valueUpdate, len(commitments))
	c2.Parameters.G1.SigmaCKK = make([][]curve.G1Affine, len(commitments))
	c2.Parameters.G2.Sigma = make([]curve.G2Affine, len(commitments))

	for j := range commitments {
		evals.G1.CKK[i] = make([]curve.G1Affine, 0, len(commitments[j].PrivateCommitted))
		c2.Parameters.G2.Sigma[j] = g2
	}

	nbCommitted := internal.NbElements(commitments.GetPrivateCommitted())

	// Evaluate PKK

	c2.Parameters.G1.PKK = make([]curve.G1Affine, 0, nbInternal+nbSecret-nbCommitted-len(commitments))
	evals.G1.VKK = make([]curve.G1Affine, 0, nbPublic+len(commitments))
	committedIterator := internal.NewMergeIterator(commitments.GetPrivateCommitted())
	nbCommitmentsSeen := 0
	for j := 0; j < nWires; j++ {
		// since as yet δ, γ = 1, the VKK and PKK are computed identically, as βA + αB + C
		var tmp curve.G1Affine
		tmp.Add(&bA[j], &aB[j])
		tmp.Add(&tmp, &C[j])
		commitmentIndex := committedIterator.IndexIfNext(j)
		isCommitment := nbCommitmentsSeen < len(commitments) && commitments[nbCommitmentsSeen].CommitmentIndex == j
		if commitmentIndex != -1 {
			evals.G1.CKK[commitmentIndex] = append(evals.G1.CKK[commitmentIndex], tmp)
		} else if j < nbPublic || isCommitment {
			evals.G1.VKK = append(evals.G1.VKK, tmp)
		} else {
			c2.Parameters.G1.PKK = append(c2.Parameters.G1.PKK, tmp)
		}
		if isCommitment {
			nbCommitmentsSeen++
		}
	}

	for j := range commitments {
		c2.Parameters.G1.SigmaCKK[j] = slices.Clone(evals.G1.CKK[j])
	}

	// Hash initial contribution
	c2.Challenge = c2.hash() // TODO remove
	return c2, evals
}

func VerifyPhase2(c0, c1 *Phase2, c ...*Phase2) error {
	contribs := append([]*Phase2{c0, c1}, c...)
	for i := 0; i < len(contribs)-1; i++ {
		if err := verifyPhase2(contribs[i], contribs[i+1]); err != nil {
			return err
		}
	}
	return nil
}

func verifyPhase2(current, contribution *Phase2) error {
	// Compute R for δ
	deltaR := genR(contribution.PublicKey.SG, contribution.PublicKey.SXG, current.Challenge[:], 1)

	// Check for knowledge of δ
	if !sameRatio(contribution.PublicKey.SG, contribution.PublicKey.SXG, contribution.PublicKey.XR, deltaR) {
		return errors.New("couldn't verify knowledge of δ")
	}

	// Check for valid updates using previous parameters
	if !sameRatio(contribution.Parameters.G1.Delta, current.Parameters.G1.Delta, deltaR, contribution.PublicKey.XR) {
		return errors.New("couldn't verify that [δ]₁ is based on previous contribution")
	}
	if !sameRatio(contribution.PublicKey.SG, contribution.PublicKey.SXG, contribution.Parameters.G2.Delta, current.Parameters.G2.Delta) {
		return errors.New("couldn't verify that [δ]₂ is based on previous contribution")
	}

	// Check for valid updates of PKK and Z using
	L, prevL := linearCombination(contribution.Parameters.G1.PKK, current.Parameters.G1.PKK)
	if !sameRatio(L, prevL, contribution.Parameters.G2.Delta, current.Parameters.G2.Delta) {
		return errors.New("couldn't verify valid updates of PKK using δ⁻¹")
	}
	Z, prevZ := linearCombination(contribution.Parameters.G1.Z, current.Parameters.G1.Z)
	if !sameRatio(Z, prevZ, contribution.Parameters.G2.Delta, current.Parameters.G2.Delta) {
		return errors.New("couldn't verify valid updates of PKK using δ⁻¹")
	}

	return nil
}

func (c *Phase2) hash() []byte {
	sha := sha256.New()
	c.writeTo(sha)
	return sha.Sum(nil)
}
