// Copyright 2020-2024 Consensys Software Inc.
// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.

// Code generated by gnark DO NOT EDIT

package mpcsetup

import (
	"bytes"
	"crypto/sha256"
	"errors"
	"fmt"
	"github.com/consensys/gnark-crypto/ecc"
	curve "github.com/consensys/gnark-crypto/ecc/bw6-633"
	"github.com/consensys/gnark-crypto/ecc/bw6-633/fr"
	"math/big"
)

// SrsCommons are the circuit-independent components of the Groth16 SRS,
// computed by the first phase.
// in all that follows, N is the domain size
type SrsCommons struct {
	G1 struct {
		Tau      []curve.G1Affine // {[œÑ‚Å∞]‚ÇÅ, [œÑ¬π]‚ÇÅ, [œÑ¬≤]‚ÇÅ, ‚Ä¶, [œÑ¬≤·¥∫‚Åª¬≤]‚ÇÅ}
		AlphaTau []curve.G1Affine // {Œ±[œÑ‚Å∞]‚ÇÅ, Œ±[œÑ¬π]‚ÇÅ, Œ±[œÑ¬≤]‚ÇÅ, ‚Ä¶, Œ±[œÑ·¥∫‚Åª¬π]‚ÇÅ}
		BetaTau  []curve.G1Affine // {Œ≤[œÑ‚Å∞]‚ÇÅ, Œ≤[œÑ¬π]‚ÇÅ, Œ≤[œÑ¬≤]‚ÇÅ, ‚Ä¶, Œ≤[œÑ·¥∫‚Åª¬π]‚ÇÅ}
	}
	G2 struct {
		Tau  []curve.G2Affine // {[œÑ‚Å∞]‚ÇÇ, [œÑ¬π]‚ÇÇ, [œÑ¬≤]‚ÇÇ, ‚Ä¶, [œÑ·¥∫‚Åª¬π]‚ÇÇ}
		Beta curve.G2Affine   // [Œ≤]‚ÇÇ
	}
}

// Phase1 in line with Phase1 of the MPC described in
// https://eprint.iacr.org/2017/1050.pdf
//
// Also known as "Powers of Tau"
type Phase1 struct {
	proofs struct { // "main" contributions
		Tau, Alpha, Beta valueUpdate
	}
	parameters SrsCommons
	Challenge  []byte // Hash of the transcript PRIOR to this participant
}

// Contribute contributes randomness to the Phase1 object. This mutates Phase1.
// p is trusted to be well-formed. The ReadFrom function performs such basic sanity checks.
func (p *Phase1) Contribute() {
	p.Challenge = p.hash()

	// Generate main value updates
	var (
		tauContrib, alphaContrib, betaContrib fr.Element
	)

	p.proofs.Tau, tauContrib = newValueUpdate(p.Challenge, 1)
	p.proofs.Alpha, alphaContrib = newValueUpdate(p.Challenge, 2)
	p.proofs.Beta, betaContrib = newValueUpdate(p.Challenge, 3)

	p.parameters.update(&tauContrib, &alphaContrib, &betaContrib)
}

// setZero instantiates the parameters, and sets all contributions to zero
func (c *SrsCommons) setZero(N uint64) {
	c.G1.Tau = make([]curve.G1Affine, 2*N-1)
	c.G2.Tau = make([]curve.G2Affine, N)
	c.G1.AlphaTau = make([]curve.G1Affine, N)
	c.G1.BetaTau = make([]curve.G1Affine, N)
	_, _, c.G1.Tau[0], c.G2.Tau[0] = curve.Generators()
}

// setOne instantiates the parameters, and sets all contributions to one
func (c *SrsCommons) setOne(N uint64) {
	c.setZero(N)
	g1, g2 := &c.G1.Tau[0], &c.G2.Tau[0]
	setG1 := func(s []curve.G1Affine) {
		for i := range s {
			s[i].Set(g1)
		}
	}
	setG2 := func(s []curve.G2Affine) {
		for i := range s {
			s[i].Set(g2)
		}
	}

	setG1(c.G1.Tau[1:])
	setG2(c.G2.Tau[1:])
	setG1(c.G1.AlphaTau)
	setG1(c.G1.BetaTau)
	c.G2.Beta.Set(g2)
}

// from the fourth argument on this just gives an opportunity to avoid recomputing some scalar multiplications
func (c *SrsCommons) update(tauUpdate, alphaUpdate, betaUpdate *fr.Element) {

	// TODO @gbotrel working with jacobian points here will help with perf.

	tauUpdates := powers(tauUpdate, len(c.G1.Tau))
	// saving 3 exactly scalar muls among millions. Not a significant gain but might as well.
	scaleG1InPlace(c.G1.Tau[1:], tauUpdates[1:]) // first element remains 1
	scaleG2InPlace(c.G2.Tau[1:], tauUpdates[1:])

	alphaUpdates := make([]fr.Element, len(c.G1.AlphaTau))
	alphaUpdates[0].Set(alphaUpdate)
	for i := range alphaUpdates {
		// let Œ±‚ÇÅ = Œ±‚ÇÄ.Œ±', œÑ‚ÇÅ = œÑ‚ÇÄ.œÑ'
		// then Œ±‚ÇÅœÑ‚ÇÅ‚Å± = (Œ±‚ÇÄœÑ‚ÇÄ‚Å±)Œ±'œÑ'‚Å±
		alphaUpdates[i].Mul(&tauUpdates[i], alphaUpdate)
	}
	scaleG1InPlace(c.G1.AlphaTau, alphaUpdates)

	betaUpdates := make([]fr.Element, len(c.G1.BetaTau))
	betaUpdates[0].Set(betaUpdate)
	for i := range betaUpdates {
		betaUpdates[i].Mul(&tauUpdates[i], betaUpdate)
	}
	scaleG1InPlace(c.G1.BetaTau, betaUpdates)

	var betaUpdateI big.Int
	betaUpdate.BigInt(&betaUpdateI)
	c.G2.Beta.ScalarMultiplication(&c.G2.Beta, &betaUpdateI)
}

// Seal performs the final contribution and outputs the final parameters.
// No randomization is performed at this step.
// A verifier should simply re-run this and check
// that it produces the same values.
// The inner workings of the random beacon are out of scope.
// WARNING: Seal modifies p, just as Contribute does.
// The result will be an INVALID Phase1 object, since no proof of correctness is produced.
func (p *Phase1) Seal(beaconChallenge []byte) SrsCommons {
	newContribs := beaconContributions(p.hash(), beaconChallenge, 3)
	p.parameters.update(&newContribs[0], &newContribs[1], &newContribs[2])
	return p.parameters
}

// VerifyPhase1 and return the SRS parameters usable for any circuit of domain size N
// beaconChallenge is the output of the random beacon
// and c are the output from the contributors
// WARNING: the last contribution object will be modified
func VerifyPhase1(N uint64, beaconChallenge []byte, c ...*Phase1) (SrsCommons, error) {
	prev := NewPhase1(N)
	for i := range c {
		if err := prev.Verify(c[i]); err != nil {
			return SrsCommons{}, err
		}
		prev = c[i]
	}
	return prev.Seal(beaconChallenge), nil
}

// Verify assumes previous is correct
func (p *Phase1) Verify(next *Phase1) error {

	challenge := p.hash()
	if len(next.Challenge) != 0 && !bytes.Equal(next.Challenge, challenge) {
		return errors.New("the challenge does not match the previous contribution's hash")
	}
	next.Challenge = challenge

	// the internal consistency of the vector sizes in next is assumed
	// so is its well-formedness i.e. Tau[0] = 1
	// it remains to check it is consistent with p
	N := len(next.parameters.G2.Tau)
	if N != len(p.parameters.G2.Tau) {
		return errors.New("domain size mismatch")
	}

	// verify updates to œÑ, Œ±, Œ≤
	if err := next.proofs.Tau.verify(pair{p.parameters.G1.Tau[1], nil}, pair{next.parameters.G1.Tau[1], nil}, challenge, 1); err != nil {
		return fmt.Errorf("failed to verify contribution to œÑ: %w", err)
	}
	if err := next.proofs.Alpha.verify(pair{p.parameters.G1.AlphaTau[0], nil}, pair{next.parameters.G1.AlphaTau[0], nil}, challenge, 2); err != nil {
		return fmt.Errorf("failed to verify contribution to Œ±: %w", err)
	}
	if err := next.proofs.Beta.verify(pair{p.parameters.G1.BetaTau[0], &p.parameters.G2.Beta}, pair{next.parameters.G1.BetaTau[0], &next.parameters.G2.Beta}, challenge, 3); err != nil {
		return fmt.Errorf("failed to verify contribution to Œ≤: %w", err)
	}

	if !areInSubGroupG1(next.parameters.G1.Tau[2:]) || !areInSubGroupG1(next.parameters.G1.BetaTau[1:]) || !areInSubGroupG1(next.parameters.G1.AlphaTau[1:]) {
		return errors.New("derived values ùîæ‚ÇÅ subgroup check failed")
	}
	if !areInSubGroupG2(next.parameters.G2.Tau[2:]) {
		return errors.New("derived values ùîæ‚ÇÇ subgroup check failed")
	}

	return multiValueUpdateCheck(
		p.parameters.G1.Tau,
		p.parameters.G2.Tau,
		p.parameters.G1.AlphaTau,
		p.parameters.G1.BetaTau,
	)
}

// multiValueUpdateCheck checks that a·µ¢‚Çä‚ÇÅ/a·µ¢ = b‚±º‚Çä‚ÇÅ/b‚±º = c‚Çñ‚Çä‚ÇÅ/c‚Çñ = d‚Çó‚Çä‚ÇÅ/d‚Çó for all applicable i,j,k,l
// in other words it checks that there is x such that a·µ¢ = x ≤a‚ÇÄ, b‚±º = x ≤b‚ÇÄ, c‚Çñ = x ≤c‚ÇÄ, d‚Çó = x ≤d‚ÇÄ
func multiValueUpdateCheck(a []curve.G1Affine, b []curve.G2Affine, c, d []curve.G1Affine) error {
	// lemma: let K be a field and
	// F = ‚àë f·µ¢‚±º X‚Å±Y ≤     F' = ‚àë f'·µ¢‚±º X‚Å±Y ≤
	// G = ‚àë g·µ¢ Z‚Å±        G' = ‚àë g'·µ¢ Z‚Å±
	// polynomials in K[X,Y,Z].
	// if F/F' = G/G'
	// then F/F' = G/G' ‚àà K
	//
	// view our polynomials in K[X,Y,Z]
	// By multiplying out the polynomials we get
	// FG' = F'G ‚áí ‚àë f·µ¢‚±ºg'‚Çñ X·∂¶Y ≤Z·µè = ‚àë f'·µ¢‚±ºg‚Çñ‚Çó X·∂¶Y ≤Z·µè
	// pick i‚ÇÄ ,j‚ÇÄ , k‚ÇÄ where f'·µ¢‚ÇÄ‚±º‚ÇÄ, g'‚Çñ‚ÇÄ ‚â† 0
	// let x ‚âî f·µ¢‚ÇÄ‚±º‚ÇÄ/f'·µ¢‚ÇÄ‚±º‚ÇÄ = g‚Çñ‚ÇÄ/g'‚Çñ‚ÇÄ
	// now for any i,j: f·µ¢‚±ºg'‚Çñ‚ÇÄ = f'·µ¢‚±ºg‚Çñ‚ÇÄ ‚áí
	// f·µ¢‚±º = x f'·µ¢‚±º
	// likewise for any i: f·µ¢‚ÇÄ‚±º‚ÇÄg'·µ¢ = f'·µ¢‚ÇÄ‚±º‚ÇÄg·µ¢ ‚áí
	// g·µ¢ = x g'·µ¢

	// now we use this to check that:
	//    1. a·µ¢ ‚âî G1.Tau[i]      = [œÑ‚Å±]‚ÇÅ
	//    2. b·µ¢ ‚âî G2.Tau[i]      = [œÑ‚Å±]‚ÇÇ
	//    3. c·µ¢ ‚âî G1.AlphaTau[i] = [Œ±œÑ‚Å±]‚ÇÅ
	//    4. d·µ¢ ‚âî G1.BetaTau[i]  = [Œ≤œÑ‚Å±]‚ÇÅ

	// construct the polynomials
	// F  ‚âî a‚ÇÄ + a‚ÇÅX + ... + a‚ÇÇ‚Çô‚Çã‚ÇÉX¬≤·¥∫‚Åª¬≥ + c‚ÇÄY + c‚ÇÅXY + ... + c‚Çô‚Çã‚ÇÇX·¥∫‚Åª¬≤Y + d‚ÇÄY¬≤ + d‚ÇÅXY¬≤ + ... + d‚Çô‚Çã‚ÇÇX·¥∫‚Åª¬≤Y¬≤
	// F' ‚âî a‚ÇÅ + a‚ÇÇX + ... + a‚ÇÇ‚Çô‚Çã‚ÇÇX¬≤·¥∫‚Åª¬≥ + c‚ÇÅY + c‚ÇÇXY + ... + c‚Çô‚Çã‚ÇÅX·¥∫‚Åª¬≤Y + d‚ÇÅY¬≤ + d‚ÇÇXY¬≤ + ... + d‚Çô‚Çã‚ÇÅX·¥∫‚Åª¬≤Y¬≤
	// G  ‚âî b‚ÇÄ + b‚ÇÅZ + ... + b‚Çô‚Çã‚ÇÇZ·¥∫‚Åª¬≤
	// G' ‚âî b‚ÇÅ + b‚ÇÇZ + ... + b‚Çô‚Çã‚ÇÅZ·¥∫‚Åª¬≤

	// if F/F' = G/G' we get F/F' = G/G' = a‚ÇÄ/a‚ÇÅ = 1/œÑ, which yields:
	// for 0 ‚â§ i ‚â§ N-2:  b·µ¢ = b·µ¢‚Çä‚ÇÅ/œÑ, c·µ¢ = c·µ¢‚Çä‚ÇÅ/œÑ, d·µ¢ = d·µ¢‚Çä‚ÇÅ/œÑ
	// for 0 ‚â§ i ‚â§ 2N-3: a·µ¢ = a·µ¢‚Çä‚ÇÅ/œÑ

	// from previous checks we already know:
	//    1. a‚ÇÄ = 1
	//    2. b‚ÇÄ = 1
	//    3. c‚ÇÄ = Œ±
	//    4. d‚ÇÄ = Œ≤
	// and so the desired results follow

	ends := partialSums(len(a), len(c), len(d))

	g1s := make([]curve.G1Affine, 0, ends[len(ends)-1])
	g1s = append(g1s, a...)
	g1s = append(g1s, c...)
	g1s = append(g1s, d...)

	g1Num, g1Denom := linearCombinationsG1(g1s, bivariateRandomMonomials(ends...), ends)
	g2Num, g2Denom := linearCombinationsG2(b, linearCombCoeffs(len(b)))

	if !sameRatioUnsafe(g1Num, g1Denom, g2Num, g2Denom) {
		return errors.New("multi-value update check failed")
	}

	return nil

}

func (p *Phase1) hash() []byte {
	sha := sha256.New()
	p.WriteTo(sha)
	sha.Write(p.Challenge)
	return sha.Sum(nil)
}

// Initialize an empty Phase1 contribution object
// to be used by the first contributor or the verifier
// N is the FFT domain size
func (p *Phase1) Initialize(N uint64) {
	if ecc.NextPowerOfTwo(N) != N {
		panic("N must be a power of 2")
	}
	p.parameters.setOne(N)
}

// NewPhase1 creates an empty Phase1 contribution object
// to be used by the first contributor or the verifier
// N is the FFT domain size
func NewPhase1(N uint64) *Phase1 {
	res := new(Phase1)
	res.Initialize(N)
	return res
}
